{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신경망 복습\n",
    "- 시그모이드 함수에 대한 설명 [링크](https://velog.io/@metterian/%EB%8C%80%EC%B2%B4-%EC%8B%9C%EA%B7%B8%EB%AA%A8%EC%9D%B4%EB%93%9CSigmoid-%ED%95%A8%EC%88%98%EA%B0%80-%EB%AD%94%EB%8D%B0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시그모이드 함수 정의\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2) # 10의 샘플 데이터, 데이터 한개에 입력이 2개 들억마\n",
    "W1 = np.random.randn(2, 4)\n",
    "b1 = np.random.randn(4)\n",
    "W2 = np.random.randn(4, 3)\n",
    "b2 = np.random.randn(3)\n",
    "\n",
    "h = np.matmul(x, W1) + b1\n",
    "a = sigmoid(h)\n",
    "s = np.matmul(a, W2) + b2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서 $x$의 형상은 (10,2)이다. 2차원 데이터 10개가 미니배치로 처리된다는 뜻이다. 그리고 최종 출력인 $s$의 형상은 (10,3)이 된다.  \n",
    "즉, 각 데이터는 **3차원** 데이터로 변환 되었다는 뜻이다. (10개의 데이터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.58092121, -0.82513827],\n",
       "       [ 0.06871648,  0.80309211],\n",
       "       [-1.79383287,  0.26074672],\n",
       "       [-0.36632401, -1.18437086],\n",
       "       [-0.72273923, -0.55746734],\n",
       "       [ 0.5921558 ,  0.33383509],\n",
       "       [-0.57090784, -0.21050217],\n",
       "       [ 1.93359996,  1.89415545],\n",
       "       [-0.51065796, -1.01660497],\n",
       "       [-0.54290326, -1.3681997 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.58068217, -1.75672898, -0.8693984 ],\n",
       "       [ 1.16058176, -1.79280375, -1.06484197],\n",
       "       [ 0.90635456, -1.5801317 , -1.04533034],\n",
       "       [ 0.58339994, -1.96449277, -0.80039834],\n",
       "       [ 0.71320792, -1.8359119 , -0.88185515],\n",
       "       [ 0.99951332, -1.95961013, -0.96901566],\n",
       "       [ 0.8230642 , -1.81510225, -0.92779118],\n",
       "       [ 1.42071959, -1.93101837, -1.15147672],\n",
       "       [ 0.60921949, -1.92484176, -0.81984863],\n",
       "       [ 0.53797658, -1.95193275, -0.78659576]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 신경망은 3차원 데이터를 출력한다. 따라서 각 차원의 값을 이용하여 3개의 클래스 분류를 할 수 있다. 이 경우, 출력된 3차원 벡터의 각 차원은 각 클래스에 대응하는 점수(score)가 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 1번째 클래스일 확률 | 2번째 클래스일 확률 | 3번째 클래스일 확률 |\n",
    "| ------------------- | ------------------- | ------------------- |\n",
    "| -1.16618845         | 1.83387914          | 1.21169264          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 계층으로 클래스화 및 순전파 구현\n",
    "신경망의 계층(Layer)를 구현 해보자. \n",
    "> 순전파(Forward Propagation): 입력층에서 출력층으로 향하는 전파\n",
    "역전파(Backward Propagatoin): 데이터(기울기)를 순전파와 반대 방향으로 전달\n",
    "\n",
    "여기서는 각 계층을 모두 클래스 형태로 구현 할 것입니다. 본 포스팅에서는 계층을 구현할 때 다음과 같은 구현 규칙을 따르겠습니다.\n",
    "- 모든 계층은 forward()와 backward() 메소드르 가진다.\n",
    "- 모든 계층은 인스터든 변수인 params와 grads를 가진다  \n",
    "  \n",
    "params: 가중치와 편향 같은 매개변수를 저장  \n",
    "grads: params에 저장된 각 매개변수에 대응하여, 해당 매개변수의 기울기를 보관"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "    def forward(self, x):\n",
    "        return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Affine 계층 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine:\n",
    "    def __init__(self, W,b):\n",
    "        self.params = [W,b] # 가중치와 편향 저장\n",
    "        \n",
    "    def forward(self, x):\n",
    "        W,b = self.params\n",
    "        out = np.matmul(x, W) + b\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affine 계층이 초기화 될때 무조건 가중치과 편향을 입력 받아 params 리스트에 저장한다. foward를 진행 할때 이 params에서 가중치와 편향을 가져와서 out을 출력 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 번 포스팅에서 구현할 신경망의 구조는 다음 그림과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig1-11.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        I,H,O = input_size, hidden_size, output_size\n",
    "        \n",
    "        # 가중치, 편향 초기화\n",
    "        W1 = np.random.randn(I, H)\n",
    "        b1 = np.random.randn(H)\n",
    "        W2 = np.random.randn(H, O)\n",
    "        b2 = np.random.randn(O)\n",
    "        \n",
    "        \n",
    "        # Layer Class 저장\n",
    "        self.layers = [\n",
    "            Affine(W1,b1),\n",
    "            Sigmoid(),\n",
    "            Affine(W2, b2)\n",
    "        ]\n",
    "        \n",
    "        # 도는 가중치를 리스트에 모은다.\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params += layer.params\n",
    "            \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(10, 2)\n",
    "model = TwoLayerNet(input_size = 2, hidden_size = 4, output_size = 3)\n",
    "s = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.27015811, -0.4424734 ,  0.66313966],\n",
       "       [-0.28848802, -0.62220867,  0.79751107],\n",
       "       [-0.65262826, -2.05965159,  1.51541516],\n",
       "       [-0.31645364, -0.95184501,  1.25626564],\n",
       "       [-0.23625201, -0.43740492,  0.78999392],\n",
       "       [-0.25103893, -0.73009047,  1.28059955],\n",
       "       [-0.27479095, -0.83981107,  1.58098757],\n",
       "       [-0.14574824,  0.02207983,  1.73395153],\n",
       "       [-0.70407324, -1.62112377,  0.86720396],\n",
       "       [-0.31683321, -1.01094901,  1.46840511]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아까와 동일하게 클래스틀 통해 3개의 클래스의 대한 값을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 신경망의 학습\n",
    "학습을 통해 좋은 신경망을 만드는 것이 딥러닝의 목적입니다. 일반적으로 추론이란, 다중 클래스 분류등의 문제에 답을 구하는 작업입니다. 한편, 딥러닝에서의 학습은 최적의 매개 변수 값을 찾는 작업입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 손실 함수\n",
    "신경망 학습에서는 학습이 얼마나 잘 되고 있는지 알기 위한 '척도'가 필요합니다. 일반적으로 학습 단계의 특정 시점에서 신경망의 성능을 나타내는 척도로 **손실**(Loss)를 사용합니다.   \n",
    "\n",
    "신경망의 손실은 **손실함수**(Loss function)을 사용해 구합니다. 특히, 분류 문제인 다중 클래스 문제(multi-class classification)문제에서는 **교차 엔트로피 오차**(Cross Entropy Error)를 사용합니다. 교차 엔트로피 오차는 딥러닝이 출력하는 각 클래스의 '확률'과 '정답 레이블'을 이용해 구할 수 있습니다.  \n",
    "\n",
    "위 위에서 만든 딥러닝 계층에 Softmax 계층과 Cross Entropy Error 계층을 새로 추가합니다. (Softmax 계층은 소프트맥스 함수를, Cross Entropy Error 계층은 교차 엔트로피 오차를 구하는 계층입니다.) 구현 하고자 하는 신경망을 나타내면 다음 그림과 같습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/fig1-12.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소프트맥스 함수\n",
    "소프트맥스 함수를 나타내면 다음과 같습니다. \n",
    "\n",
    "$$\n",
    "y_k = \\frac{exp(S_k)}{\\sum_{i=1}^{n}exp(S_i)}\n",
    "$$\n",
    "\n",
    "위 식을 해석하면 출력이 총 $n$ 개일 때, $k$번째 출력 $y_k$를 구하는 계산식입니다.  $y_k$는 $k$번째 클래스에 속할 확률을 나타냅니다.  \n",
    "\n",
    "소프트맥스 함수의 출력의 각 원소는 $0≤y_k≤1$의 법위 값을 갖습니다. 이 원소들을 모두 더하면 1.0이 됩니다. 즉, 소프트맥스를 통하면 확률로 해석이 가능해진다는 것이죠. 소프트맥스 함수를 통해 출력을 받으면 이는 바로 교차 엔트로피 오차 계층으로 들어가게 됩니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 교차 엔트로피 오차\n",
    "이때 교차 엔트로피 오차의 수식은 다음과 같습니다.\n",
    "\n",
    "$$\n",
    "L = -\\sum_{k}t_klogy_k\n",
    "$$\n",
    "\n",
    "여기서 $t_k$는 $k$번째 클래스에 해당하는 정답 레이블(=[0,0,1]) 입니다. $log$는 네이피어 상수(혹은 오일러 수) $e$를 밑으로 하는 로그 입니다.   \n",
    "\n",
    "나아가 미니배치를 고려하면 교차 엔트로피 오차의 식은 다음과 같습니다. 이 식에서 데이터는 $N$개이며, $t_nk$는 $n$번재 데이터의 $k$차원째의 값을 의미합니다. 그리고 $y_nk$는 신경망의 출력이고, $t_nk$는 정답 레이블 입니다. \n",
    "\n",
    "$$\n",
    "L = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_klogy_k\n",
    "$$\n",
    "\n",
    "위 식에서는 $N$으로 나눠서 1개당 '**평균 손실 함수**'를 구합니다. 이렇게 평균을 구함으로써 미니배치의 크게이 관계없이 항상 일관된 척도를 얻을 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def croos_entropy_error(y, t):\n",
    "    if n.dim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 정답 데이터가 원핫 벅터일 경우 정답 레이블 인덱스로 변환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "        \n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    return - np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x - x.max(axis=1, keepdims=True)\n",
    "        x = np.exp(x)\n",
    "        x /= x.sum(axis=1, keepdims=True)\n",
    "        \n",
    "    else x.ndim == 1:\n",
    "        x = x - np.max(x)\n",
    "        x = np.exp(x) / np.sum(np.exp(x))\n",
    "    \n",
    "\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.params, self.grads = [], []\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "        \n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(np.arange(1,10).reshape(3,3), keepdims=0).ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5108256237659907"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
